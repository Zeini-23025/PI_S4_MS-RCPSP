import os
import json
import csv
import networkx as nx
from collections import defaultdict

# R√©pertoires
RESULTATS_DIR = "resultats"
INSTANCES_DIR = "instances"
FORD_FULKERSON_DIR = os.path.join(RESULTATS_DIR, "ford_fulkerson")

# Les 10 algorithmes √† tester
ALGOS = ["EFT", "HRPW*", "HRU1", "HRU2", "LFT", "LST", "MTS", "STFD", "TIMRES", "TIMROS"]

# Dur√©es fixes des activit√©s (√† adapter √† ton probl√®me)
durations = [
    3, 2, 4, 3, 2, 1, 2, 4, 1, 2, 1, 3,
    2, 3, 2, 1, 2, 3, 1, 2, 3, 2
]

def build_resource_constraint_graph(ordered_activities, durations, num_resources=1):
    """
    Construit un graphe de flot pour mod√©liser les contraintes de ressources.
    Chaque activit√© n√©cessite une ressource pendant sa dur√©e d'ex√©cution.
    """
    G = nx.DiGraph()
    source = "source"
    sink = "sink"
    
    # Calcul des temps de d√©but et de fin pour chaque activit√©
    start_times = {}
    end_times = {}
    current_time = 0
    
    for i, act in enumerate(ordered_activities):
        duration = durations[act - 1]
        start_times[act] = current_time
        end_times[act] = current_time + duration
        current_time += duration
    
    # Cr√©ation des n≈ìuds temporels
    all_times = set()
    for act in ordered_activities:
        all_times.add(start_times[act])
        all_times.add(end_times[act])
    all_times = sorted(all_times)
    
    # Ajout de la source et du sink
    G.add_node(source)
    G.add_node(sink)
    
    # Pour chaque activit√©, cr√©er des arcs
    for act in ordered_activities:
        activity_node = f"act_{act}"
        G.add_node(activity_node)
        
        # Arc de la source vers l'activit√© (capacit√© = 1)
        G.add_edge(source, activity_node, capacity=1)
        
        # Arc de l'activit√© vers le sink (capacit√© = 1)
        G.add_edge(activity_node, sink, capacity=1)
    
    return G, source, sink

def build_precedence_graph(ordered_activities, durations):
    """
    Construit un graphe de flot bas√© sur les contraintes de pr√©c√©dence.
    Le max flow repr√©sente le nombre maximum d'activit√©s pouvant √™tre ex√©cut√©es.
    """
    G = nx.DiGraph()
    source = "source"
    sink = "sink"
    
    # Ajouter tous les n≈ìuds d'activit√©s
    for act in ordered_activities:
        G.add_node(f"act_{act}")
    
    # Connecter la source √† toutes les activit√©s
    for act in ordered_activities:
        G.add_edge(source, f"act_{act}", capacity=1)
    
    # Connecter toutes les activit√©s au sink
    for act in ordered_activities:
        G.add_edge(f"act_{act}", sink, capacity=1)
    
    # Ajouter les contraintes de pr√©c√©dence bas√©es sur l'ordre
    for i in range(len(ordered_activities) - 1):
        current_act = ordered_activities[i]
        next_act = ordered_activities[i + 1]
        # Pas de contrainte suppl√©mentaire car l'ordre est d√©j√† respect√©
    
    return G, source, sink

def build_time_expanded_graph(ordered_activities, durations):
    """
    Construit un graphe temporel √©tendu pour mod√©liser l'utilisation des ressources dans le temps.
    """
    G = nx.DiGraph()
    source = "source"
    sink = "sink"
    
    # Calcul de la dur√©e totale du projet
    total_duration = sum(durations[act - 1] for act in ordered_activities)
    
    # Cr√©ation des n≈ìuds de temps
    time_nodes = []
    for t in range(total_duration + 1):
        time_node = f"time_{t}"
        time_nodes.append(time_node)
        G.add_node(time_node)
    
    # Connexion entre n≈ìuds de temps cons√©cutifs (capacit√© = nombre de ressources)
    for t in range(total_duration):
        G.add_edge(f"time_{t}", f"time_{t+1}", capacity=1)
    
    # Ajout des activit√©s
    current_time = 0
    for act in ordered_activities:
        duration = durations[act - 1]
        start_time = current_time
        end_time = current_time + duration
        
        activity_node = f"act_{act}"
        G.add_node(activity_node)
        
        # Connexion source -> activit√©
        G.add_edge(source, activity_node, capacity=1)
        
        # Connexion activit√© -> temps de d√©but
        G.add_edge(activity_node, f"time_{start_time}", capacity=1)
        
        # Connexion temps de fin -> sink
        G.add_edge(f"time_{end_time}", sink, capacity=1)
        
        current_time = end_time
    
    return G, source, sink

def compute_max_flow_multiple_methods(ordered_activities, durations):
    """
    Calcule le max flow avec plusieurs m√©thodes pour validation crois√©e.
    Retourne les r√©sultats et la valeur principale du max flow.
    """
    results = {}
    max_flow = 0  # Variable principale pour stocker le max flow
    
    # M√©thode 1: Graphe simple de pr√©c√©dence
    try:
        G1, source1, sink1 = build_precedence_graph(ordered_activities, durations)
        flow1, _ = nx.maximum_flow(G1, source1, sink1, flow_func=nx.algorithms.flow.edmonds_karp)
        results['precedence'] = flow1
        max_flow = max(max_flow, flow1)  # Mise √† jour du max flow
    except Exception as e:
        results['precedence'] = f"Erreur: {e}"
    
    # M√©thode 2: Graphe de contraintes de ressources
    try:
        G2, source2, sink2 = build_resource_constraint_graph(ordered_activities, durations)
        flow2, _ = nx.maximum_flow(G2, source2, sink2, flow_func=nx.algorithms.flow.edmonds_karp)
        results['resource'] = flow2
        max_flow = max(max_flow, flow2)  # Mise √† jour du max flow
    except Exception as e:
        results['resource'] = f"Erreur: {e}"
    
    # M√©thode 3: Calcul direct bas√© sur la faisabilit√©
    try:
        # Pour un ordonnancement s√©quentiel, le max flow est le nombre d'activit√©s
        # qui peuvent √™tre effectivement ex√©cut√©es
        feasible_activities = len([act for act in ordered_activities if act <= len(durations)])
        results['direct'] = feasible_activities
        max_flow = max(max_flow, feasible_activities)  # Mise √† jour du max flow
    except Exception as e:
        results['direct'] = f"Erreur: {e}"
    
    return results, max_flow

def analyze_schedule_quality(ordered_activities, durations):
    """
    Analyse la qualit√© de l'ordonnancement en calculant diff√©rentes m√©triques.
    """
    metrics = {}
    
    # Dur√©e totale du projet
    total_duration = sum(durations[act - 1] for act in ordered_activities if act <= len(durations))
    metrics['total_duration'] = total_duration
    
    # Nombre d'activit√©s valides
    valid_activities = [act for act in ordered_activities if 1 <= act <= len(durations)]
    metrics['valid_activities'] = len(valid_activities)
    metrics['invalid_activities'] = len(ordered_activities) - len(valid_activities)
    
    # Utilisation des ressources (supposant 1 ressource)
    if total_duration > 0:
        metrics['resource_utilization'] = len(valid_activities) / total_duration
    else:
        metrics['resource_utilization'] = 0
    
    return metrics

def load_ordered_activities_from_json(filepath):
    """Charge les activit√©s ordonn√©es depuis un fichier JSON."""
    try:
        with open(filepath, 'r') as f:
            data = json.load(f)
        return data.get("ordered_activities", [])
    except Exception as e:
        print(f"Erreur lors du chargement de {filepath}: {e}")
        return []

def export_results_to_csv(all_instances_results, output_path):
    """
    Exporte les r√©sultats de tous les algorithmes dans un fichier CSV pour comparaison.
    """
    headers = ["Instance", "Meilleur_Algorithme", "Flot_Maximal"] + ALGOS

    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(headers)

        for instance_data in all_instances_results:
            instance = instance_data["instance"]
            best_algo = instance_data["best_algorithm"]
            best_flow = instance_data["best_max_flow"]
            results = instance_data["algorithms_results"]

            row = [instance, best_algo, best_flow]
            for algo in ALGOS:
                if algo in results:
                    max_flow_value = results[algo]["max_flow"]
                    row.append(max_flow_value)
                else:
                    row.append("N/A")
            writer.writerow(row)

    print(f"üìä R√©sultats export√©s vers : {output_path}")

def export_detailed_csv(all_instances_results, output_path):
    """
    Exporte un CSV d√©taill√© avec toutes les m√©triques pour analyse approfondie.
    """
    headers = ["Instance", "Algorithme", "Max_Flow", "Duree_Totale", 
               "Activites_Valides", "Activites_Invalides", "Utilisation_Ressources"]
    
    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(headers)
        
        for instance_data in all_instances_results:
            instance = instance_data["instance"]
            results = instance_data["algorithms_results"]
            
            for algo in ALGOS:
                if algo in results:
                    data = results[algo]
                    row = [
                        instance,
                        algo,
                        data["max_flow"],
                        data["metrics"]["total_duration"],
                        data["metrics"]["valid_activities"],
                        data["metrics"]["invalid_activities"],
                        round(data["metrics"]["resource_utilization"], 4)
                    ]
                    writer.writerow(row)
    
    print(f"üìã R√©sultats d√©taill√©s export√©s vers : {output_path}")

def load_dzn_instance(filepath):
    """
    Charge les dur√©es d'activit√©s depuis un fichier DZN.
    Retourne une liste de dur√©es (int).
    """
    if not os.path.isfile(filepath):
        return []
    durations = []
    try:
        with open(filepath, 'r') as f:
            for line in f:
                # Recherche une ligne contenant 'duree' ou 'durations'
                if 'duree' in line or 'durations' in line:
                    # Extrait les nombres entre crochets ou accolades
                    import re
                    match = re.search(r'=\s*[\[\{]([^\]\}]*)[\]\}]', line)
                    if match:
                        values = match.group(1)
                        durations = [int(x) for x in values.replace(',', ' ').split()]
                        break
    except Exception as e:
        print(f"Erreur lors du chargement de {filepath}: {e}")
        return []
    return durations

def get_instances_from_dzn():
    """
    Retourne la liste des noms d'instances (sans extension) pr√©sents dans le dossier INSTANCES_DIR.
    """
    instances = []
    if not os.path.isdir(INSTANCES_DIR):
        return instances
    for filename in os.listdir(INSTANCES_DIR):
        if filename.endswith(".dzn"):
            instance_name = filename[:-4]  # Retire '.dzn'
            instances.append(instance_name)
    return instances

def main():
    """Fonction principale pour traiter toutes les instances."""
    os.makedirs(FORD_FULKERSON_DIR, exist_ok=True)

    # D√©tection des instances DZN
    dzn_instances = get_instances_from_dzn()
    print(f"üîç Instances DZN d√©tect√©es : {len(dzn_instances)}")
    if dzn_instances:
        print(f"  Instances trouv√©es : {', '.join(dzn_instances[:5])}{'...' if len(dzn_instances) > 5 else ''}")

    # D√©tection des instances avec r√©sultats JSON
    instances_with_results = set()
    for algo in ALGOS:
        algo_dir = os.path.join(RESULTATS_DIR, algo)
        if not os.path.isdir(algo_dir):
            continue
        for filename in os.listdir(algo_dir):
            if filename.endswith(".json"):
                instance_name = filename[:-5]
                instances_with_results.add(instance_name)

    print(f"üîç Instances avec r√©sultats JSON : {len(instances_with_results)}")
    
    # Intersection des instances DZN et des r√©sultats
    common_instances = set(dzn_instances) & instances_with_results
    print(f"üéØ Instances communes (DZN + r√©sultats) : {len(common_instances)}")
    
    if not common_instances:
        print("‚ö†Ô∏è Aucune instance commune trouv√©e. V√©rifiez les noms des fichiers.")
        # Affichage pour debugging
        print(f"Instances DZN : {dzn_instances[:3]}...")
        print(f"Instances JSON : {sorted(list(instances_with_results))[:3]}...")
        return
    
    # Stockage des r√©sultats pour export CSV
    all_instances_results = []
    
    # Traitement de chaque instance
    for instance in sorted(common_instances):
        print(f"\nüß™ Traitement de l'instance : {instance}")
        
        # Chargement des dur√©es depuis le fichier DZN
        dzn_filepath = os.path.join(INSTANCES_DIR, f"{instance}.dzn")
        instance_durations = load_dzn_instance(dzn_filepath)
        
        if not instance_durations:
            print(f"‚ö†Ô∏è Impossible de charger les dur√©es pour {instance}, utilisation des dur√©es par d√©faut")
            instance_durations = durations  # Utilise les dur√©es par d√©faut
        else:
            print(f"‚úÖ Dur√©es charg√©es depuis {instance}.dzn : {len(instance_durations)} activit√©s")
        
        results = {}
        
        for algo in ALGOS:
            filepath = os.path.join(RESULTATS_DIR, algo, f"{instance}.json")
            if not os.path.isfile(filepath):
                print(f"  ‚ö†Ô∏è Fichier manquant pour {algo}")
                continue
                
            ordered = load_ordered_activities_from_json(filepath)
            if not ordered:
                print(f"  ‚ö†Ô∏è Donn√©es vides pour {algo}")
                continue
            
            # Calcul des max flows avec les dur√©es de l'instance
            max_flows, max_flow = compute_max_flow_multiple_methods(ordered, instance_durations)
            
            # Analyse de la qualit√© de l'ordonnancement
            metrics = analyze_schedule_quality(ordered, instance_durations)
            
            results[algo] = {
                "max_flow": max_flow,  # Variable principale du max flow
                "max_flows_details": max_flows,  # D√©tails par m√©thode
                "metrics": metrics,
                "ordered_activities": ordered
            }
            
            print(f"  {algo:8} ‚Üí Max Flow = {max_flow}")
            print(f"          ‚Üí D√©tails: {max_flows}")
            print(f"          ‚Üí M√©triques: dur√©e={metrics['total_duration']}, "
                  f"activit√©s_valides={metrics['valid_activities']}")

        if not results:
            print(f"‚ö†Ô∏è Aucune donn√©e pour l'instance {instance}.")
            continue

        # Analyse comparative - Utilisation de la variable max_flow principale
        best_algo = None
        best_max_flow = 0
        
        for algo, data in results.items():
            if data['max_flow'] > best_max_flow:
                best_max_flow = data['max_flow']
                best_algo = algo
        
        # Analyse d√©taill√©e par m√©thode
        comparison = {}
        for method in ['precedence', 'resource', 'direct']:
            method_results = {}
            for algo, data in results.items():
                if method in data['max_flows_details'] and isinstance(data['max_flows_details'][method], (int, float)):
                    method_results[algo] = data['max_flows_details'][method]
            
            if method_results:
                best_algo_method = max(method_results.items(), key=lambda x: x[1])[0]
                comparison[method] = {
                    "best_algorithm": best_algo_method,
                    "best_value": method_results[best_algo_method],
                    "all_values": method_results
                }

        # Sauvegarde des r√©sultats
        summary = {
            "instance": instance,
            "best_algorithm": best_algo,
            "best_max_flow": best_max_flow,
            "algorithms_results": results,
            "comparison_by_method": comparison,
            "analysis_info": {
                "total_algorithms": len(results),
                "activity_duration_list": instance_durations,
                "dzn_file": f"{instance}.dzn"
            }
        }
        
        # Ajout aux r√©sultats globaux pour CSV
        all_instances_results.append(summary)

        out_path = os.path.join(FORD_FULKERSON_DIR, f"{instance}.json")
        with open(out_path, 'w') as f_out:
            json.dump(summary, f_out, indent=2)

        print(f"‚úÖ R√©sultats sauvegard√©s dans {out_path}")
        print(f"üèÜ MEILLEUR ALGORITHME: {best_algo} avec MAX FLOW = {best_max_flow}")
        
        # Affichage du r√©sum√© d√©taill√©
        for method, comp in comparison.items():
            print(f"üìä M√©thode {method}: Meilleur = {comp['best_algorithm']} "
                  f"(valeur = {comp['best_value']})")
    
    # Export des r√©sultats en CSV
    if all_instances_results:
        # CSV de comparaison principal
        csv_comparison_path = os.path.join(FORD_FULKERSON_DIR, "comparaison_algorithmes.csv")
        export_results_to_csv(all_instances_results, csv_comparison_path)
        
        # CSV d√©taill√©
        csv_detailed_path = os.path.join(FORD_FULKERSON_DIR, "resultats_detailles.csv")
        export_detailed_csv(all_instances_results, csv_detailed_path)
        
        # Statistiques globales
        print(f"\nüìà STATISTIQUES GLOBALES:")
        print(f"  ‚Ä¢ Nombre total d'instances trait√©es: {len(all_instances_results)}")
        
        # Comptage des victoires par algorithme
        algo_wins = defaultdict(int)
        for result in all_instances_results:
            if result["best_algorithm"]:
                algo_wins[result["best_algorithm"]] += 1
        
        print(f"  ‚Ä¢ Victoires par algorithme:")
        for algo in sorted(algo_wins.keys()):
            print(f"    - {algo}: {algo_wins[algo]} victoires")
        
        # Algorithme le plus performant globalement
        if algo_wins:
            best_global_algo = max(algo_wins.items(), key=lambda x: x[1])
            print(f"  ü•á Algorithme le plus performant: {best_global_algo[0]} ({best_global_algo[1]} victoires)")
    
    else:
        print("‚ö†Ô∏è Aucune donn√©e √† exporter.")

if __name__ == "__main__":
    main()