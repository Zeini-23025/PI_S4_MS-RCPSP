# üß™ Tests et Validation

## üìã Vue d'ensemble

Le syst√®me MS-RCPSP dispose de **plusieurs niveaux de tests** pour assurer la fiabilit√©, la performance et la correctness des algorithmes d'ordonnancement et du machine learning.

## üöÄ Tests automatis√©s principaux

### **1. `run_project.py` - Test complet du syst√®me**
```bash
python3 run_project.py
```

**√âtapes de validation** :
1. ‚úÖ **D√©pendances** : V√©rification et installation automatique
2. ‚úÖ **Donn√©es** : G√©n√©ration des makespans (20 instances)
3. ‚úÖ **ML** : Entra√Ænement du mod√®le (88.9% accuracy moyenne)
4. ‚úÖ **Tests** : Validation sur 5 projets repr√©sentatifs
5. ‚úÖ **Graphiques** : Cr√©ation des visualisations
6. ‚úÖ **Rapport** : Validation finale des r√©sultats

**Crit√®res de r√©ussite** :
- 6/6 √©tapes compl√©t√©es avec succ√®s
- Mod√®le ML g√©n√©r√© (~930KB)
- Taux de succ√®s IA ‚â• 80%
- Graphiques cr√©√©s sans erreurs

### **2. `solution_finale.py` - Tests de validation**
```bash
python3 solution_finale.py
```

**Tests effectu√©s** :
- üîß **Diagnostic syst√®me** : Mod√®le, dossiers, permissions
- üß† **Test ML** : Chargement et fonctionnement du mod√®le
- üéØ **Tests repr√©sentatifs** : 5 instances vari√©es
- üìä **M√©triques** : Performance IA et algorithmes
- üìÑ **Validation r√©sultats** : Format et coh√©rence

**Sortie attendue** :
```
üéØ Tests r√©ussis: 5
ü§ñ IA correcte: 5/5 (100.0%)
üèÜ Algorithmes optimaux trouv√©s: LPT, LST, EST
üìä Makespans: Min 106, Max 143, Moyenne 127.8 jours
```

## üß™ Tests unitaires par module

### **1. Tests du moteur d'ordonnancement**

#### **Test `msrcpsp_final.py`**
```python
def test_algorithm_validity():
    """Test la validit√© des algorithmes d'ordonnancement"""
    
    # Donn√©es de test
    test_instance = "Instances/MSLIB_Set1_4799.msrcp"
    algorithms = ['EST', 'LFT', 'MSLF', 'SPT', 'LPT', 'FCFS', 'LST']
    
    for algo in algorithms:
        makespan = solve_instance(test_instance, algo)
        
        # V√©rifications
        assert makespan > 0, f"Makespan invalide pour {algo}"
        assert makespan < 1000, f"Makespan suspicieux pour {algo}"
        assert isinstance(makespan, (int, float)), f"Type invalide pour {algo}"
```

**Commande de test** :
```bash
python3 -c "
from msrcpsp_final import solve_instance
test_file = 'Instances/MSLIB_Set1_4799.msrcp'
for algo in ['EST', 'LFT', 'SPT']:
    result = solve_instance(test_file, algo)
    print(f'{algo}: {result} jours')
    assert result > 0
print('‚úÖ Tests moteur d\'ordonnancement r√©ussis')
"
```

### **2. Tests du Machine Learning**

#### **Test extraction de caract√©ristiques**
```python
def test_feature_extraction():
    """Test l'extraction des 43 caract√©ristiques"""
    
    from binary_relevance_msrcpsp import extract_features
    
    # Test sur instance connue
    features = extract_features("Instances/MSLIB_Set1_4799.msrcp")
    
    # V√©rifications
    assert len(features) == 43, "Nombre de features incorrect"
    assert all(isinstance(f, (int, float)) for f in features), "Types invalides"
    assert all(f >= 0 for f in features), "Valeurs n√©gatives suspectes"
    
    return True
```

#### **Test mod√®le ML**
```python
def test_ml_model():
    """Test le mod√®le de machine learning"""
    
    import joblib
    from binary_relevance_msrcpsp import MLIntegratedMSRCPSP
    
    # Charger le mod√®le
    model_path = "./resultats/binary_relevance_model.pkl"
    ml_solver = MLIntegratedMSRCPSP(model_path)
    
    # Test de pr√©diction
    test_instance = "Instances/MSLIB_Set1_4799.msrcp"
    result = ml_solver.solve_with_ml_guidance(test_instance, "./resultats_ml/")
    
    # V√©rifications
    assert 'ml_recommended_algorithms' in result
    assert 'best_algorithm' in result
    assert 'best_makespan' in result
    assert len(result['ml_recommended_algorithms']) > 0
    
    return True
```

**Commande de test** :
```bash
python3 -c "
from binary_relevance_msrcpsp import MLIntegratedMSRCPSP
ml = MLIntegratedMSRCPSP('./resultats/binary_relevance_model.pkl')
print('‚úÖ Mod√®le ML charg√© avec succ√®s')
"
```

### **3. Tests de g√©n√©ration de donn√©es**

#### **Test `makespan_calculator.py`**
```python
def test_data_generation():
    """Test la g√©n√©ration des donn√©es d'entra√Ænement"""
    
    import os
    import json
    
    # Ex√©cuter la g√©n√©ration (sur un petit √©chantillon)
    os.system("python3 makespan_calculator.py")
    
    # V√©rifier les r√©sultats
    details_dir = "./resultats/makespan_details"
    assert os.path.exists(details_dir), "Dossier de r√©sultats manquant"
    
    json_files = [f for f in os.listdir(details_dir) if f.endswith('.json')]
    assert len(json_files) > 0, "Aucun fichier de r√©sultats g√©n√©r√©"
    
    # V√©rifier le format d'un fichier
    with open(os.path.join(details_dir, json_files[0])) as f:
        data = json.load(f)
    
    required_keys = ['instance', 'results', 'timestamp']
    for key in required_keys:
        assert key in data, f"Cl√© manquante : {key}"
    
    return True
```

## üìä Tests de performance

### **1. Tests de temps d'ex√©cution**

#### **Benchmark individuel**
```python
import time

def benchmark_algorithm(instance_path, algorithm):
    """Benchmark d'un algorithme sur une instance"""
    
    start_time = time.time()
    makespan = solve_instance(instance_path, algorithm)
    end_time = time.time()
    
    execution_time = end_time - start_time
    
    return {
        'algorithm': algorithm,
        'makespan': makespan,
        'execution_time': execution_time,
        'instance': instance_path
    }
```

#### **Test de performance ML**
```python
def benchmark_ml_performance():
    """Test des performances du syst√®me ML"""
    
    import time
    from binary_relevance_msrcpsp import MLIntegratedMSRCPSP
    
    ml_solver = MLIntegratedMSRCPSP("./resultats/binary_relevance_model.pkl")
    
    instances = ["Instances/MSLIB_Set1_4799.msrcp", 
                "Instances/MSLIB_Set1_5060.msrcp"]
    
    total_time = 0
    correct_predictions = 0
    
    for instance in instances:
        start_time = time.time()
        result = ml_solver.solve_with_ml_guidance(instance, "./resultats_ml/")
        end_time = time.time()
        
        total_time += (end_time - start_time)
        
        # V√©rifier si l'IA √©tait correcte
        if result['best_algorithm'] in result['ml_recommended_algorithms']:
            correct_predictions += 1
    
    avg_time = total_time / len(instances)
    accuracy = correct_predictions / len(instances) * 100
    
    print(f"Temps moyen par instance : {avg_time:.2f}s")
    print(f"Pr√©cision IA : {accuracy:.1f}%")
    
    return avg_time, accuracy
```

**Crit√®res de performance attendus** :
```
Temps par instance      : < 1 seconde
Temps total syst√®me     : < 60 secondes
Pr√©cision IA           : > 80%
M√©moire utilis√©e       : < 500 MB
```

### **2. Tests de charge**

#### **Test sur volume important**
```bash
# Test sur 100 instances (si disponibles)
python3 -c "
from binary_relevance_msrcpsp import MLIntegratedMSRCPSP
import os
import time

ml = MLIntegratedMSRCPSP('./resultats/binary_relevance_model.pkl')
instances = [f for f in os.listdir('Instances/') if f.endswith('.msrcp')][:100]

start_time = time.time()
success_count = 0

for i, instance in enumerate(instances):
    try:
        result = ml.solve_with_ml_guidance(f'Instances/{instance}', './resultats_ml/')
        success_count += 1
        if i % 10 == 0:
            print(f'Trait√© {i+1}/{len(instances)} instances')
    except Exception as e:
        print(f'Erreur sur {instance}: {e}')

end_time = time.time()
total_time = end_time - start_time

print(f'‚úÖ Test de charge termin√©')
print(f'Instances trait√©es: {success_count}/{len(instances)}')
print(f'Temps total: {total_time:.1f}s')
print(f'Temps moyen: {total_time/success_count:.2f}s par instance')
"
```

## üéØ Tests de r√©gression

### **1. Validation des r√©sultats de r√©f√©rence**

#### **R√©sultats attendus (instances de r√©f√©rence)**
```python
EXPECTED_RESULTS = {
    "MSLIB_Set1_4799.msrcp": {
        "EST": 143.0,
        "LPT": 143.0, 
        "LST": 143.0,
        "SPT": 143.0,
        "FCFS": 149.0
    },
    "MSLIB_Set1_5060.msrcp": {
        "EST": 115.0,
        "LPT": 115.0,
        "LST": 115.0,
        "SPT": 115.0,
        "FCFS": 115.0
    }
}

def test_regression():
    """Test de non-r√©gression sur instances de r√©f√©rence"""
    
    for instance, expected in EXPECTED_RESULTS.items():
        instance_path = f"Instances/{instance}"
        
        for algorithm, expected_makespan in expected.items():
            actual_makespan = solve_instance(instance_path, algorithm)
            
            assert actual_makespan == expected_makespan, \
                f"R√©gression d√©tect√©e : {instance} {algorithm} " \
                f"attendu {expected_makespan}, obtenu {actual_makespan}"
    
    print("‚úÖ Tests de r√©gression r√©ussis")
```

### **2. Validation des performances ML**

#### **M√©triques de r√©f√©rence**
```python
ML_BASELINE = {
    "accuracy_minimum": 80.0,      # 80% minimum
    "precision_moyenne": 85.0,     # 85% en moyenne
    "recall_minimum": 70.0,        # 70% minimum
    "f1_score_minimum": 75.0       # 75% minimum
}

def validate_ml_metrics():
    """Valide que les m√©triques ML sont au niveau attendu"""
    
    # Charger les r√©sultats d'√©valuation
    with open("./resultats/ml_evaluation_metrics.json") as f:
        metrics = json.load(f)
    
    for metric, expected_value in ML_BASELINE.items():
        if metric in metrics:
            actual_value = metrics[metric]
            assert actual_value >= expected_value, \
                f"Performance ML d√©grad√©e : {metric} " \
                f"attendu >={expected_value}, obtenu {actual_value}"
    
    print("‚úÖ Validation des m√©triques ML r√©ussie")
```

## üîß Tests de robustesse

### **1. Tests avec donn√©es invalides**

#### **Gestion des erreurs**
```python
def test_error_handling():
    """Test la gestion des erreurs et cas limites"""
    
    from binary_relevance_msrcpsp import MLIntegratedMSRCPSP
    
    ml_solver = MLIntegratedMSRCPSP("./resultats/binary_relevance_model.pkl")
    
    # Test fichier inexistant
    try:
        result = ml_solver.solve_with_ml_guidance("fichier_inexistant.msrcp", "./resultats_ml/")
        assert False, "Erreur attendue pour fichier inexistant"
    except FileNotFoundError:
        print("‚úÖ Gestion fichier inexistant OK")
    
    # Test dossier de sortie inexistant
    try:
        result = ml_solver.solve_with_ml_guidance("Instances/MSLIB_Set1_4799.msrcp", "/dossier/inexistant/")
        # Doit cr√©er le dossier automatiquement
        print("‚úÖ Cr√©ation automatique dossier OK")
    except Exception as e:
        print(f"‚ö†Ô∏è Probl√®me cr√©ation dossier : {e}")
```

### **2. Tests de limites**

#### **Instances extr√™mes**
```python
def test_edge_cases():
    """Test sur des cas limites"""
    
    # Test sur la plus petite instance
    smallest_files = sorted([f for f in os.listdir("Instances/") if f.endswith('.msrcp')])[:1]
    
    # Test sur la plus grande instance  
    largest_files = sorted([f for f in os.listdir("Instances/") if f.endswith('.msrcp')], 
                          key=lambda x: os.path.getsize(f"Instances/{x}"))[-1:]
    
    test_files = smallest_files + largest_files
    
    for test_file in test_files:
        try:
            result = solve_instance(f"Instances/{test_file}", "EST")
            assert result > 0, f"Makespan invalide pour {test_file}"
            print(f"‚úÖ Test {test_file} : {result} jours")
        except Exception as e:
            print(f"‚ùå √âchec {test_file} : {e}")
```

## üìä Tests d'int√©gration

### **1. Test workflow complet**
```python
def test_complete_workflow():
    """Test du workflow complet du syst√®me"""
    
    import os
    import tempfile
    
    # Cr√©er un environnement de test temporaire
    with tempfile.TemporaryDirectory() as temp_dir:
        
        # 1. G√©n√©ration des donn√©es
        print("1. Test g√©n√©ration donn√©es...")
        # Code pour g√©n√©rer des donn√©es dans temp_dir
        
        # 2. Entra√Ænement ML
        print("2. Test entra√Ænement ML...")
        # Code pour entra√Æner un mod√®le
        
        # 3. Tests de pr√©diction
        print("3. Test pr√©dictions...")
        # Code pour tester les pr√©dictions
        
        # 4. G√©n√©ration de graphiques
        print("4. Test visualisations...")
        # Code pour g√©n√©rer des graphiques
        
        print("‚úÖ Workflow complet test√© avec succ√®s")
```

### **2. Test d'int√©gration des modules**
```bash
# Test s√©quentiel de tous les modules
python3 -c "
print('Test 1: G√©n√©ration donn√©es')
exec(open('makespan_calculator.py').read())

print('Test 2: ML')  
exec(open('binary_relevance_msrcpsp.py').read())

print('Test 3: Validation')
exec(open('solution_finale.py').read())

print('‚úÖ Tous les modules int√©gr√©s fonctionnent')
"
```

## üéØ Tests de validation m√©tier

### **1. Tests de coh√©rence des r√©sultats**

#### **Coh√©rence algorithmique**
```python
def test_algorithm_consistency():
    """Test la coh√©rence des r√©sultats entre algorithmes"""
    
    instance = "Instances/MSLIB_Set1_4799.msrcp"
    algorithms = ['EST', 'LFT', 'MSLF', 'SPT', 'LPT', 'FCFS', 'LST']
    
    results = {}
    for algo in algorithms:
        results[algo] = solve_instance(instance, algo)
    
    # V√©rifications de coh√©rence
    min_makespan = min(results.values())
    max_makespan = max(results.values())
    
    # L'√©cart ne doit pas √™tre trop important (facteur 2 max)
    assert max_makespan <= min_makespan * 2, \
        f"√âcart trop important : {min_makespan} vs {max_makespan}"
    
    # Au moins un algorithme doit √™tre optimal
    optimal_count = sum(1 for v in results.values() if v == min_makespan)
    assert optimal_count >= 1, "Aucun algorithme optimal trouv√©"
    
    print(f"‚úÖ Coh√©rence v√©rifi√©e : √©cart {max_makespan/min_makespan:.2f}x")
```

#### **Coh√©rence ML**
```python
def test_ml_consistency():
    """Test la coh√©rence des recommandations ML"""
    
    from binary_relevance_msrcpsp import MLIntegratedMSRCPSP
    
    ml_solver = MLIntegratedMSRCPSP("./resultats/binary_relevance_model.pkl")
    
    # Tester la m√™me instance plusieurs fois
    instance = "Instances/MSLIB_Set1_4799.msrcp"
    
    results = []
    for i in range(3):
        result = ml_solver.solve_with_ml_guidance(instance, "./resultats_ml/")
        results.append(result['ml_recommended_algorithms'])
    
    # Les recommandations doivent √™tre identiques (mod√®le d√©terministe)
    for i in range(1, len(results)):
        assert results[i] == results[0], \
            f"Recommandations ML inconsistantes : {results[0]} vs {results[i]}"
    
    print("‚úÖ Coh√©rence ML v√©rifi√©e")
```

## üìã Rapport de tests automatis√©

### **Script de test complet**
```bash
#!/bin/bash
# test_complete.sh

echo "üß™ LANCEMENT DES TESTS COMPLETS MS-RCPSP"
echo "========================================"

# Test 1: Installation et d√©pendances
echo "Test 1: V√©rification des d√©pendances..."
python3 -c "import numpy, sklearn, matplotlib, pandas, seaborn; print('‚úÖ OK')" || exit 1

# Test 2: Modules de base
echo "Test 2: Modules de base..."
python3 -c "from msrcpsp_final import solve_instance; print('‚úÖ OK')" || exit 1

# Test 3: G√©n√©ration de donn√©es
echo "Test 3: G√©n√©ration de donn√©es..."
timeout 60 python3 makespan_calculator.py || exit 1

# Test 4: Machine Learning
echo "Test 4: Machine Learning..."
echo "1" | timeout 120 python3 binary_relevance_msrcpsp.py || exit 1

# Test 5: Validation syst√®me
echo "Test 5: Validation syst√®me..."
timeout 60 python3 solution_finale.py || exit 1

# Test 6: Visualisations
echo "Test 6: Visualisations..."
echo "4" | timeout 180 python3 nettoyage_et_graphiques.py || exit 1

echo ""
echo "üéâ TOUS LES TESTS R√âUSSIS !"
echo "=========================="
echo "‚úÖ Syst√®me compl√®tement op√©rationnel"
```

### **M√©triques de succ√®s**
```
Tests pass√©s        : 6/6 (100%)
Temps total         : < 10 minutes
Mod√®le ML g√©n√©r√©    : ‚úÖ (~930KB)
R√©sultats ML        : ‚úÖ (5+ fichiers)
Graphiques cr√©√©s    : ‚úÖ (4+ images)
Pr√©cision IA        : ‚úÖ (‚â•80%)
Performance temps   : ‚úÖ (<1s/instance)
```
